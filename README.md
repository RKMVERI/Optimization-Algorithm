# Optimization-Algorithm
For learning OPTAL

Visualization of the Machine Learning optimization algorithms are shown here with example of different functions and with comparison among them.


* Line search & direction search in Gradient Descent:
+     Exact Methods:
*         Using derivatives ( for differentiable fnctons only):
*             Newton's method
*             Secant Method
*         Without using derivatives (requre function evaluation only):
*             Golden Section
*             Fibonacci
*             Bisection
*     Inexat method  (step length found is not exactly optimal )  {these are parameter specific}:
*         Armijo
*         Wolf-Powell

Here All optimization techniques are explained through code. You can find the codes inside the ipynb files along with some explaination. The codes are very simple to understand, if the theory is clear to you. ( In future I will put some easy explaination and theory here )



Also I think, it will be better if anyone want to help me by just making the ipynb files more understable by seperation the topics.
If you feel hard anywhere, contact me in mahendranandi.0608@gmail.com
